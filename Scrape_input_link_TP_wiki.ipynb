{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7kukyoBSa3dg5qGkx1ONH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiyaNaik12/Knowledge_graphs/blob/main/Scrape_input_link_TP_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RoSaDaZOr_y",
        "outputId": "e9c8e1f4-1069-4570-e24d-91ed66911273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=aef003208069b2f77a9dde132d690c007c7b7bba3a3b612369dcd9123eb60d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YFrVeg19ghJ",
        "outputId": "8d17ec0d-cc9b-4335-eb06-3151b423743f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.element.Tag'>\n",
            "<class 'bs4.element.ResultSet'>\n",
            "921\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import html\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Ai\"\n",
        "response = requests.get(url)\n",
        "bs = BeautifulSoup(response.content, \"html.parser\")\n",
        "links1 = set()\n",
        "links = bs.find('div',class_ = 'mw-content-container')\n",
        "print(type(links))\n",
        "links1 = links.find_all('a')\n",
        "print(type(links1))\n",
        "\n",
        "print(len(links1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of Wikipedia URLs to scrape\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Ai\",\n",
        "    \"https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence\"\n",
        "]\n",
        "\n",
        "# Set to store all unique links\n",
        "unique_links = set()\n",
        "\n",
        "# Loop through the URLs\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Find the mw-content-container div\n",
        "    content_container = soup.find(\"div\", {\"class\": \"mw-content-container\"})\n",
        "\n",
        "    # Find all the links within the content container\n",
        "    links = content_container.find_all(\"a\")\n",
        "\n",
        "    # Filter and collect valid links\n",
        "    count = 0\n",
        "    for link in links:\n",
        "        href = link.get(\"href\")\n",
        "        # Exclude specific links and links that point to files/images\n",
        "        if (href and href.startswith(\"/wiki/\") and\n",
        "            not href.startswith((\"/wiki/File:\",\"/wiki/Talk:\",\"/wiki/Template:\",\"/wiki/Help:\", \"/wiki/Special:\"))) and count < 30:\n",
        "            full_link = \"https://en.wikipedia.org\" + href\n",
        "            unique_links.add(full_link)\n",
        "            count += 1\n",
        "\n",
        "# Save the unique links to a file\n",
        "with open(\"/content/input_link_TP_wiki.txt\", \"w\") as file:\n",
        "    for unique_link in unique_links:\n",
        "        file.write(f\"{unique_link}\\n\")\n",
        "\n",
        "print(\"Unique links saved to file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPZPqbDKu-0Z",
        "outputId": "1dd1f6b8-d69f-4cc7-e06a-9f701edf51a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique links saved to file.\n"
          ]
        }
      ]
    }
  ]
}